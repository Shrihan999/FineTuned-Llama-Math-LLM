# ðŸ§® Fine-Tuned Llama Math Solver â€“ Advanced Math Problem-Solving LLM

## ðŸ“Œ Overview  
The **Fine-Tuned Llama Math Solver** is a domain-specialized Large Language Model (LLM) designed to **accurately solve complex math problems**.  
It builds on the **Llama-3.2 3B base model**, fine-tuned on the extensive **MetaMath QA dataset** with 395,000+ question-answer pairs.  
Optimized with **4-bit quantization**, **LoRA fine-tuning**, and **gradient checkpointing** to deliver high accuracy while running efficiently on limited hardware.

---

## ðŸš€ Problem Statement  
Solving advanced math problems using general LLMs often results in **inaccurate or incomplete answers** due to insufficient domain specialization.  
Existing math-solving models may lack scalability or require extensive compute resources for fine-tuning and inference.

**Goal:**  
To develop a **fine-tuned math problem-solving LLM** that:  
- Provides **accurate and reliable answers** across diverse math topics.  
- Runs efficiently using model optimization techniques.  
- Supports local inference for privacy and low-latency use cases.

---

## ðŸ’¡ Proposed Solution  
This project implements a fine-tuned Llama-3.2 3B model that:  
1. Is fine-tuned on the **MetaMath QA dataset** with 395k+ math QA pairs.  
2. Uses **LoRA (Low-Rank Adaptation)** and **gradient checkpointing** to reduce training overhead.  
3. Applies **4-bit quantization** to reduce model size without sacrificing performance.  
4. Is evaluated on a custom **multiple-choice benchmark of 1,000 questions** for robust accuracy assessment.

---

## ðŸ›  Tech Stack  
- **Dataset:** MetaMath QA Dataset (~395k question-answer pairs)  
- **Base Model:** Llama-3.2 3B  
- **Fine-Tuning:** LoRA + Gradient Checkpointing  
- **Quantization:** 4-bit (for efficient inference)  
- **Framework:** PyTorch / Hugging Face Transformers  
- **Evaluation:** Custom multiple-choice benchmark  

---

## ðŸ”„ Workflow  
1. **Data Preparation** â€“ Load and preprocess MetaMath QA dataset.  
2. **Fine-Tuning** â€“ Adapt the base Llama model using LoRA with gradient checkpointing.  
3. **Quantization** â€“ Convert model weights to 4-bit precision for efficient inference.  
4. **Evaluation** â€“ Test on custom 1,000-question multiple-choice set to validate accuracy improvements.  
5. **Deployment** â€“ Provide inference scripts for running locally on user machines.

---

## ðŸ“‚ System Architecture  
- **Data Layer:** MetaMath QA dataset + custom evaluation set.  
- **Model Layer:** Llama-3.2 3B fine-tuned with LoRA and quantized to 4-bit.  
- **Inference Layer:** Python scripts to load model and answer math questions.  

---

## ðŸ“‚ Project Structure
```
.
fine-tuned-llama-math/
â”‚
â”œâ”€â”€ .gitignore                 # Specifies files for Git to ignore
â”œâ”€â”€ README.md                  # Project documentation
â”œâ”€â”€ app.log                    # Log file generated by the main script
â”œâ”€â”€ main.py                    # Main script to run the pipeline (process, train, start)
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ train.py                   # (Alternative) Script to directly run model training
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml            # Main configuration file for paths, models, and training
â”‚
â”œâ”€â”€ data/                      # Directory for datasets (created automatically)
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â””â”€â”€ (Processed and tokenized data will be saved here)
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ (Raw downloaded datasets can be stored here)
â”‚
â”œâ”€â”€ logs/                      # Directory for logs (created automatically)
â”‚   â””â”€â”€ training/
â”‚       â””â”€â”€ (TensorBoard and other training logs will be saved here)
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ fine-tuned-llama-3.2-3b/ # Output directory for the fine-tuned model
â”‚       â”œâ”€â”€ checkpoint-*/      # Checkpoints saved during training
â”‚       â””â”€â”€ final/             # The final saved model after training completes
â”‚
â””â”€â”€ src/                       # Main source code directory
    â”œâ”€â”€ __init__.py
    â”‚
    â”œâ”€â”€ data_processing/       # Scripts for data loading and processing
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ data_processor.py
    â”‚   â””â”€â”€ dataset_loader.py
    â”‚
    â”œâ”€â”€ inference/             # Scripts for model inference and response generation
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ model_inference.py
    â”‚
    â”œâ”€â”€ model/                 # Scripts related to model loading (base and fine-tuned)
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ model_loader.py
    â”‚
    â”œâ”€â”€ training/              # Scripts for the model training and fine-tuning process
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ trainer.py
    â”‚
    â””â”€â”€ ui/                    # Scripts for the Streamlit user interface
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ app.py             # Main Streamlit application file
        â””â”€â”€ chat_interface.py  # Defines the chat UI components
```

---


## ðŸ“Š Results  
- Increased accuracy from **61% (base model)** to **79%** on the custom math benchmark.  
- Efficient model size and inference due to quantization and LoRA.  
- Robust math problem-solving capabilities across diverse topics.

---



## ðŸ“¦ Installation & Usage

```bash
# Clone the repository
git clone https://github.com/<your-username>/fine-tuned-llama-math-solver.git
cd fine-tuned-llama-math-solver

# (Optional) Create and activate virtual environment
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
.\venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

#pre-processing
python main.py --process

#Training the mdoel
python main.py --train

#chatbot
python main.py --start
