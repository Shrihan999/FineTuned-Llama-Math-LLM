model:
  name: "meta-llama/Llama-3.2-1B"
  max_length: 512
  temperature: 0.7
  top_p: 0.9
  num_beams: 1

training:
  batch_size: 8  # Increased for RTX 4070
  gradient_accumulation_steps: 4  # Reduced since we increased batch size
  learning_rate: 5e-4
  num_epochs: 1
  warmup_ratio: 0.05
  weight_decay: 0.01
  max_steps: 1000
  eval_steps: 100
  save_steps: 200
  logging_steps: 10
  fp16: true  # Enable mixed precision training
  
qlora:
  r: 8
  lora_alpha: 16
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
  lora_dropout: 0.05

data:
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  max_samples: 10000  # Keeping 10k samples for faster iteration

paths:
  raw_data: "data/raw"
  processed_data: "data/processed"
  model_output: "models"
  logs: "logs"